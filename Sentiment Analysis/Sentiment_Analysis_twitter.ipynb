{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to analyse the user sentiment towards the US airline industry on twitter using different models from the scikit-learn library.\n",
    "- the dataset used contains a manually labelled sentiment per tweet : (positive, negative, neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the libraries needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and inspecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_num</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@JetBlue thank you for incredible customer svc...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@united I was well taken care of, thanks. I've...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@united Too Late Flight, damage has been done....</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@USAirways of course! \"Yeah, travel has gotten...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@united I'm very frustrated and have wasted 2 ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instance_num                                              tweet sentiment\n",
       "0             1  @JetBlue thank you for incredible customer svc...  positive\n",
       "1             2  @united I was well taken care of, thanks. I've...  positive\n",
       "2             3  @united Too Late Flight, damage has been done....  negative\n",
       "3             4  @USAirways of course! \"Yeah, travel has gotten...  negative\n",
       "4             5  @united I'm very frustrated and have wasted 2 ...  negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   instance_num  4000 non-null   int64 \n",
      " 1   tweet         4000 non-null   object\n",
      " 2   sentiment     4000 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 93.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sentiment'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = 'dataset_airlines.tsv'\n",
    "df_dataset = pd.read_table(dataset, usecols=[0,1,2], names=['instance_num', 'tweet', 'sentiment'], header=None)\n",
    "df_dataset.head(5)\n",
    "print('')\n",
    "#checking the data, checking for Missing Values and the type of data\n",
    "df_dataset.info(verbose=True)\n",
    "#checking the distribution of the sentiments, to see if the dataset is balanced\n",
    "freq_sentiment = pd.crosstab(index=df_dataset['sentiment'], columns='count')\n",
    "freq_sentiment.plot.bar(y='count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the tweets.\n",
    "The tweets must be cleaned due to the special characters. Only strings and the characters: @, # would be mantained in the original tweets. Urls present in a tweet are also deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet_text):\n",
    "    '''input: unformatted tweets list\n",
    "       output: formmatted tweets list '''\n",
    "    \n",
    "    tweet_text_split = [] \n",
    "    for tweet in tweet_text: \n",
    "        tweet_text_split.append(tweet.split())\n",
    "\n",
    "    tweet_text_split_cleaned = [] \n",
    "    for i in range(len(tweet_text_split)):\n",
    "        aux_tweet = []\n",
    "        for word in tweet_text_split[i]:\n",
    "            x = re.sub('^https?:\\/\\/.*[\\r\\n]*', '', word) #for deleting urls\n",
    "            x = ''.join(re.findall('[a-zA-Z\\#@]',x)) #for only considering letters, @,#\n",
    "            if len(x) > 1: #len of tweet at least 2\n",
    "                aux_tweet.append(x)\n",
    "        tweet_text_split_cleaned.append(aux_tweet)\n",
    "    tweet_text_formatted = [' '.join(i) for i in tweet_text_split_cleaned]\n",
    "    return tweet_text_formatted\n",
    "\n",
    "df_dataset['tweet'] = clean_tweet(df_dataset['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into a training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training size is: 3200 tweets\n",
      "The test size is: 800 tweets\n"
     ]
    }
   ],
   "source": [
    "X = df_dataset['tweet'].to_list()\n",
    "y = df_dataset['sentiment'].to_list()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "print(f'The training size is: {len(X_train)} tweets')\n",
    "print(f'The test size is: {len(X_test)} tweets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating the tweets\n",
    "Different functions from the nltk library are tried in order to normalize the words in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_normalize(dataset, method):\n",
    "    '''method: Stemmming or Lemmatization'''\n",
    "    \n",
    "    if method == 'Stemming':\n",
    "        porter = PorterStemmer()\n",
    "    if method == 'Lemmatization': #default by noun\n",
    "        lemma = WordNetLemmatizer()\n",
    "    \n",
    "    normalized_tweet = []\n",
    "    for tweet in dataset:\n",
    "        normalized_sentence = []\n",
    "        word_tokens = tweet.split()\n",
    "        for w in word_tokens:\n",
    "            if method == 'Stemming':\n",
    "                normalized_sentence.append(porter.stem(w))\n",
    "            if method == 'Lemmatization':\n",
    "                normalized_sentence.append(lemma.lemmatize(w))\n",
    "        normalized_tweet.append(normalized_sentence)\n",
    "        \n",
    "    normalized_tweet = [' '.join(i) for i in normalized_tweet]\n",
    "    return(normalized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying lemmatization\n",
    "X_train = nltk_normalize(X_train,'Lemmatization')\n",
    "X_test = nltk_normalize(X_test,'Lemmatization')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the tweets.\n",
    "The sentences of each tweet are transformed into a vector that contains the count of each word of the tweet present in the total bags of words. By doing this encoding , it is possible to train different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_dataset(X_train,X_test, max_features, lowercase=True, token_pattern='[#\\@\\w]+'):\n",
    "    vectorize = CountVectorizer(max_features=max_features, lowercase=lowercase, token_pattern=token_pattern) \n",
    "    vectorizer_train = vectorize.fit_transform(X_train)\n",
    "    vectorizer_test = vectorize.transform(X_test)\n",
    "    return vectorizer_train, vectorizer_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create count vectorizer and fit it with training data, from the #example provided\n",
    "vectorizer_train, vectorizer_test = vectorize_dataset(X_train, X_test,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of the first tweet of X_train vectorized into a (1,1000), where 1000 is the max_features (the 1000 most frequent words in X_train)\n",
    "vectorizer_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training different models\n",
    "Models that are going to be tested: **Decision Tree**, **Multinomial Naive Bayes** and **Compliment Naive Bayes**\n",
    "- **Decision Tree** is a common model used for classification that normally works correctly\n",
    "- **Multinomial Naive Bayes** is a model normally used for text classification\n",
    "- **The Compliment Naive Bayes** is a model that is used when a dataset is unbalance. This is normal to happen in sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model, X_test, y_test):\n",
    "    predicted_y = model.predict(X_test)\n",
    "    #print(model.predict_proba(X_test))\n",
    "    print(classification_report(y_test, predicted_y,zero_division=0))\n",
    "    #print(confusion_matrix(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "\n",
    "def check_models(X_train,X_test,y_train,y_test, models_tested):\n",
    "    for model in models_tested:\n",
    "        print(f'Classification report for the model: {model}')\n",
    "        if model == 'DT':\n",
    "            selection = tree.DecisionTreeClassifier(min_samples_leaf=0.01,criterion='entropy',random_state=0)\n",
    "                    \n",
    "        if model == 'MNB':\n",
    "            selection = MultinomialNB()\n",
    "        \n",
    "        if model == 'CNB':\n",
    "            selection = ComplementNB()\n",
    "            \n",
    "        model = selection.fit(X_train, y_train)\n",
    "        model_predict(model, X_test, y_test)\n",
    "        print('')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for the model: DT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.93      0.82       492\n",
      "     neutral       0.47      0.25      0.33       160\n",
      "    positive       0.74      0.45      0.56       148\n",
      "\n",
      "    accuracy                           0.70       800\n",
      "   macro avg       0.65      0.54      0.57       800\n",
      "weighted avg       0.68      0.70      0.67       800\n",
      "\n",
      "\n",
      "Classification report for the model: MNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.88      0.84       492\n",
      "     neutral       0.60      0.51      0.55       160\n",
      "    positive       0.70      0.59      0.64       148\n",
      "\n",
      "    accuracy                           0.76       800\n",
      "   macro avg       0.70      0.66      0.68       800\n",
      "weighted avg       0.75      0.76      0.75       800\n",
      "\n",
      "\n",
      "Classification report for the model: CNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.83      0.84       492\n",
      "     neutral       0.61      0.58      0.60       160\n",
      "    positive       0.63      0.71      0.67       148\n",
      "\n",
      "    accuracy                           0.76       800\n",
      "   macro avg       0.70      0.71      0.70       800\n",
      "weighted avg       0.76      0.76      0.76       800\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_tested = ['DT', 'MNB', 'CNB']\n",
    "check_models(vectorizer_train, vectorizer_test, y_train, y_test, models_tested)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
